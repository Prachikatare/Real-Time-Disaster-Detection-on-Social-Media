# -*- coding: utf-8 -*-
"""Disaster_project_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15hrQOInKxSgsRRIia3Mbuh3JjtBlGwA4
"""

# Install required libraries
!pip install nltk==3.8.1 lime==0.2.0.1 pygwalker==0.4.8 pivottablejs==0.9.0 numba==0.59.0
!pip install ydata-profiling==4.7.0  # Modern replacement for pandas_profiling

# Import libraries
import numpy as np  # linear algebra
import pandas as pd  # data processing, CSV file I/O
import os
import matplotlib.pyplot as plt
import seaborn as sb
import re
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer
import warnings
warnings.filterwarnings("ignore")
from nltk.corpus import stopwords
from collections import Counter, defaultdict
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.metrics import f1_score
import lime
from lime import lime_text
from lime.lime_text import LimeTextExplainer
import pygwalker as pyg
from pivottablejs import pivot_ui
import numba
from ydata_profiling import ProfileReport  # Updated import

# Initialize lemmatizer
wordnet_lemmatizer = WordNetLemmatizer()

from sklearn.metrics import accuracy_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score, accuracy_score

# Download NLTK corpora
import nltk
nltk.download('wordnet')
nltk.download('stopwords')

# Load and profile datasets
train = pd.read_csv('/content/train.csv')
train_profile = ProfileReport(train, title="train")
train_profile.to_notebook_iframe()

# Define the clean_text function from your code
def clean_text(text):
    emoji_pattern = re.compile("["
                               u"\U0001F600-\U0001F64F"  # emoticons
                               u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                               u"\U0001F680-\U0001F6FF"  # transport & map symbols
                               u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                               u"\U00002702-\U000027B0"
                               u"\U000024C2-\U0001F251"
                               "]+", flags=re.UNICODE)
    text = emoji_pattern.sub(r'', text)

    url = re.compile(r'https?://\S+|www\.\S+')
    text = url.sub(r'', text)

    text = text.replace('#', ' ')
    text = text.replace('@', ' ')
    symbols = re.compile(r'[^A-Za-z0-9 ]')
    text = symbols.sub(r'', text)

    return text

# Clean and lemmatize keyword function
def clean_and_lemmatize_keyword(keyword):
    if keyword == 'None':  # Handle filled NaN values
        return keyword
    # Replace %20 with space
    keyword = keyword.replace('%20', ' ')
    # Split multi-word keywords (e.g., "radiation emergency") and lemmatize each
    words = keyword.split()
    lemmatized = [wordnet_lemmatizer.lemmatize(word.lower()) for word in words]
    return ' '.join(lemmatized)

# Preprocess data
train['text'] = train['text'].apply(clean_text)
train['keyword'] = train['keyword'].fillna('None').apply(clean_and_lemmatize_keyword)
train['location'] = train['location'].fillna('None')

# Combine features
X = train['text'] + ' ' + train['keyword']
y = train['target']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Vectorize
vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'))
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

# Define and evaluate models
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Multinomial Naive Bayes': MultinomialNB(),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)
}

for name, model in models.items():
    model.fit(X_train_vec, y_train)
    y_pred = model.predict(X_test_vec)
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average='binary')
    print(f"\n{name}:")
    print(f"Accuracy on test data: {accuracy:.4f}")
    print(f"F1 Score on test data: {f1:.4f}")

# Choose the best model (By F1 Score , using Logistic Regression)
best_model = LogisticRegression(max_iter=1000)
best_model.fit(X_train_vec, y_train)

# Function to predict disaster based on input text and keyword
def predict_disaster(text_input, keyword_input="None"):
    # Clean the input text and keyword
    cleaned_text = clean_text(text_input)
    cleaned_keyword = clean_and_lemmatize_keyword(keyword_input)

    # Combine features
    combined_input = cleaned_text + ' ' + cleaned_keyword

    # Vectorize the input
    input_vec = vectorizer.transform([combined_input])

    # Predict
    prediction = best_model.predict(input_vec)[0]

    # Return appropriate message based on prediction
    if prediction == 1:
        return "Your request is accepted. Help will arrive soon!"
    else:
        return "This does not appear to be an emergency situation."

# Example usage with interactive input
def user_interface():
    print("Enter details to check if it's a disaster situation:")
    text = input("Enter the text (e.g., tweet or message): ")
    keyword = input("Enter a keyword (optional, press Enter for none): ") or "None"

    result = predict_disaster(text, keyword)
    print("\nResult:", result)

# Run the interface
if __name__ == "__main__":
    user_interface()

"""Text: 'Help my house is on fire', Keyword: 'fire'
 ‘Great day at the beach’ with no keyword—it correctly says,
"""